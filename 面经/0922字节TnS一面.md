## 0922字节TnS一面

#### 自我介绍

- 

#### 八股

- 图网络节点学习嵌入的方式？
  - deep walk ，随机游走序列然后进行类似word2vec的学习
  - node2vec
  - 矩阵分解
- word2vec的原理？
  - ，，
- word2vec的输入是什么？
  - 独热向量，面试官说使用了lookup
  - 权重矩阵就是就是一个look up table，任何一个独热向量乘以这个look up table就得到了一个词向量 
  - 这里拓展一下BERT的lookup
    - 将三个得到的embedding（token_embedding，position_embedding，segment_embedding）分别进行look up，和上面类似，embedding id 进行onehot之后，然后再和embedding matrix 进行矩阵相乘
- word2vec如何学习，什么是梯度下降？
  - 梯度是函数上升最快的方向，目标是学习到loss的全局最小值
- 有哪些优化算法？
  - SGD
  - 动量
  - Adagrad
  - Adam

- 解释一下动量的优化算法？
  - 记住前一步的梯度方向，收敛更快更稳定
- 怎么缓解过拟合？
  - 增加数据
  - 正则化
  - 神经网络裁剪，dropout
- 为什么增加数据可以缓解过拟合？
  - 因为样本不够会学习到这批样本的特征，$P(X|\theta)$ 从极大似然估计的角度这里估计的参数只是能够保证这个条件概率最大，样本不够自然参数估计不准
- ​	正则化？
  - L1正则
  - L2正则

#### 手撕代码

- https://blog.csdn.net/Sophis_ticate/article/details/110686917

