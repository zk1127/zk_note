别人面经：

逻辑回归可以用在线性不可分吗？(我说，不能)那怎么才能让逻辑回归能用？ 
 SVM为啥映射到高维空间就能线性可分了？ 
 解决过拟合的方法 
 这个[项目]()为啥用CNN和RNN,CNN有几层，效果怎么样？为啥不用[机器学习]()的方法？会有所提高吗？有了解textcnn,textrnn么？ 
 梯度消失和梯度爆炸说一下，怎么解决？ 
 rf为啥泛化能力好？ 
 说一下决策树 
 写一下tfidf公式，含义是？ 
 说一下nltk 
 你这个[项目]()做多久了，到哪一步了，有实现吗？你觉得bert一定比Word2vec好吗？为什么？你怎么做特定领域的分词？ 
 说一下nlp的文本分类流程？(余弦距离，卡方检验，互信息区别)

word2Vec和GloVe区别

word2vec是NNLM的一个较为典型的代表，其利用了DNN的方法来训练获取到词向量，而且词向量是中间产物，这个思路最早是Bengio提出，Google  Brain提出的word2vec让词向量火了起来。而GloVe是采用词频统计信息，利用一些数学方法逐步优化得来，它没有神经网络的结构，所以词向量训练的速度相对更快。（这里当时不记得具体的公式推导了，原论文我倒是看过，但是当时记得不清了，实际上GloVe是词共现矩阵+类SVD的方法）



**一面：**主要想了解我做过的事情。

1、自我介绍。听完他总结了下，我的三个主要工作，都是文本分类。

2、让我细讲一下新闻立场任务。

延伸出来的问题：

（1）讲一下Sentence-BERT。

我说一个基于预训练过的BERT的孪生网络。在此基础上，用了三个任务微调，以更加适应句子对的特征表示。（面试官说这里可以算做掌握文本匹配技术）。

（2）Transformer的亮点。

我说Transformer比起RNN他们用了Self-Attention。（面试官说并不，Attention这个技术其实不是Transformer提出来的，而且在机器翻译上的针对RNN的一些改进，比如实现了并行，我不需要等上一个词的隐态学出来，再学下一个词，而是我可以同时学第一个词、第五个词这样子。）

（3）我们知道BERT用的Transformer的encode层，那谁用了Transformer的decode层呢。

我说GPT。（没有再继续让我讲原理之类的，只问我了解**GPT2、GPT3**吗，我说不是很了解）

（4）知道什么文本生成之类的算法吗。

我说之前看论文了解一点。**GAN和VAE**。（他接着问，你觉得能清楚的描述出来吗。我不能。他说，在这个地方，面试官一般希望你做过或者清楚的掌握原理。如果遇到刁钻的面试官，可能就...）

3、会pytorch，那你会tensorflow吗？我说会Tensorflow2。

3、反问环节

（1）做什么业务？

- 文本安全：比如直播弹幕上，屏蔽污言秽语。
- 生成广告文案。
- 客服：涉及到文本匹配。文本生成。
- 其他业务需要NLP这边做的，比如分词，热词，句向量表示。

（2）我说如果能加入，需要提前掌握啥技术？

他说给我的建议不是只针对能不能加入他们，而且所有厂的NLP。你需要提前了解的一些！@#￥：

- NLP相关技术：前沿的、工业界常用的，baseline以及最新技术都要掌握，入职拿到任务就可以比较快的上手。
- 掌握数据工程：科研上的数据集可能是直接提供给你的比如excel，但工业界的数据一般是从数据库中调的，相关的东西要掌握。
- 掌握部署工程：pytorch、tensorflow一些通讯协议相关的技术。



