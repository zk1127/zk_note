##### 百度的面试三面是一面过了就二面，二面过了就三面，基本上一天走完流程，但是百度面试需要预约和等待。手撕代码比较简单

### 百度一面

- 自我介绍
- 你的模型中用到了词向量和句向量，你能描述一下他们吗？
  - 因为用到了sentense-bert, 回答了bert相关
  - bert的架构是transformer编码器，输入是三种embedding
  - 训练目标是MLM和NSP
- bert怎么得出句向量的？
  - 回答了[CLS]可以得出句向量(蒙的)
- bert经过注意力层，会有多少参数？
  - 也是蒙的，只记得会拆为 12 × 64 × 64
  - 然后主要提及了一下$W_Q,W_K,W_V$
  - 面试官认为我这样回答也不算错？？？？？？？
- 输入bert里面的句子是不是任意两个地方都会计算注意力权重？
  - 是的
- 你用到了命名实体识别？你能说一下吗？
  - 回答了命名实体识别的方法，传统基于HMM，MEMM，CRF做
  - 然后提到了15年百度发的BiLSTM+CRF，最后说了一下最近都是BERT+CRF
  - 描述了BiLSTM或者BERT只是为了求解观测状态到隐状态之间的概率分布
  - 而CRF用来约束隐状态之间的转移概率分布
- 追问约束隐状态之间的转移概率分布是什么意思？
  - 回答了$P(y_i|y_{i-1},y_{i+1})$,面试官很满意这个回复，然后继续追问
- 看来你对概率图模型有些了解，你能继续讲解几个吗？
  - 回答，概率图模型包括朴素贝叶斯，HMM，MEMM，CRF，LDA等等
  - 回答了自己对LDA比较了解，就详细描述了LDA的假设和概率图
  - 然后回答了LDA的求解，Gibbs采样
- LDA与pLSA的区别？
  - 一个是贝叶斯派的思想，一个是频率派的思想
  - LDA是在pLSA上加了一层贝叶斯框架
- 问一个简单的问题，怎么解决过拟合？
  - 增加数据
  - 正则化
  - dropout
  - 神经网络的话减少层数
- 手撕代码，反转链表
  - 过



### 百度二面

- 自我介绍
- 死扣了简历中的细节，n次被怼
- 讲解一下对LDA的理解
  - 同上
- LDA与pLSA的区别？
  - 同上
- 在pLSA之前还有LSA你怎么理解？
  - 蒙了一个非负矩阵分解，好像蒙对了
- 图神经网络的理解？
  - 谈了一个$D^{-1}\tilde{A}XW$
  - 又扯了一下傅利叶变换
- 手撕代码
  - 最小路径和
  - 寻找二叉树子节点中的父节点

### 百度三面

- 自我介绍
- 只问了项目
- 问得最多的是，你为什么没考虑这么做？