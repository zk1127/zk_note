0806字节算法一面

- 自我介绍
- 介绍百度项目
  - 面试官表示自己是百度NLP部门出来的，很了解百度的流程，认为我的项目无法上线到大搜
- 实验项目
  - 描述一下LDA
  - LDA用什么采样方式？
    - 吉比斯采样
  - 这种采样是不是很EM算法很像，你还知道那些用了EM算法吗？
    - 答了变分推断，面试官说比如HMM
- 为什么logistics回归要用交叉熵损失，而不用MSE？
  - 先回答交叉熵损失是从极大似然估计推导出来的log似然，然后回答分类一般用交叉熵，回归用MSE
  - 面试官解答，交叉熵会使得问题是一个凸优化问题，MSE可能导致无法优化

- 为什么BERT用的是位置向量编码，而不是Transformer的相对位置编码？

  - 回答的是训练的目标不一样，Transformer目标是序列编码，BERT是掩码模型
  - 面试官解答其实是参数量的问题

- 手撕代码

  ```
  词典建立和文本序列化
  文本序列化是NLP中常见的工作。输入src_text和target_text，完成以下建立词典和文本序列化工作：
  将src_text作为输入数据， 建立英文单词词表。(<OOV>表示袋外词， 表示不存在当前词表中的词)
  将词表按index从小到大排序。
  通过1建立的词表，将target_text进行序列化
  样例说明：
  首先将src_text作为输入， 建立词表word dict， 并打印。
  根据word_dict， 将target_text进行序列化
  输入：
  src_text = "who are you"
  target_text = "where are you"
  输出：
  word dict is :
  <OOV> 0
  who 1
  are 2
  you 3
  word id list is: [0, 2, 3]
  ```

  

- 编辑距离

  ```
  给定两个单词word1和word2，请计算将word1转换为word2至少需要多少步操作。
  你可以对一个单词执行以下3种操作：
  a）在单词中插入一个字符
  b）删除单词中的一个字符
  c）替换单词中的一个字符
  ```

- QA
  - 面试官的部门在做三个任务
    - 属性建模（地理词条）
    - 定位
    - 用户画像