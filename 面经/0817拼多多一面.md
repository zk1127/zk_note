#### 自我介绍

- 项目

#### 基础知识

- 你是做自然语言处理的，你讲一下自然语言处理的发展，以及影响力重大的paper和模型？
  - 没有准备过这种问题，大概从三条线回答这个问题
  - wordnet, NNLM , word2vec 从词向量的角度的一条线
  - BERT, GPT, Erine，从语法语义的一条线
  - HMM, CRF, RNNs, Transformer，从序列建模的一条线
- 讲一讲Transformer的论文？
  - 回答主要是一下几个角度
    - 注意力机制的背景，解决无法捕捉依赖的问题
    - 任务是机器翻译，所以是encoder-decoder模型
    - 模型主要组成：attention，layer normalization ， 残差连接
- 讲一讲注意力机制，自注意力机制，多头注意力机制的区别
  - 注意力机制，答的QKV
  - 自注意力机制也答得QKV
  - 面试官说你这说的有区别嘛
    - 我其实忘了两者的区别，就开始瞎说："自注意力机制更注意捕捉自身内部的特征",可能答案就说这个
  - 多头就说捕捉多层面的注意力
- 论文里怎么证明注意力机制有效？
  - 我不记得了，回答了注意力权重，然后又扯了热力图
- 讲一讲Norm, BatchNorm,LayerNorm?
  - 传统Norm，有两种$x-x_{max}/x -x_{min}$, $x - \mu/\sigma$​ 
  - BatchNorm，突出每个Batch中对每个维度进行维度层面的归一化
  - LayerNorm，层内进行Normalization
- 为什么NLP不用BatchNorm，而用LayerNorm？
  - 因为序列长度影响可能导致每个Batch中维度不一致
- 为什么LayerNorm是合理的？比如房价预测，地理位置，面积，布局，这些特征一起做一个归一化，在NLP上是合理的吗？
  - 我解释，认为在每一维度可能学到不同层面的特征，比如语法，语义，这些特征是又相互关系的，将其归一化，可能更好的表示向量（胡说的，但是面试官没有追问）
- 聊一聊百度的实习
  - 为什么要做特征工程，而不直接端到端？
    - 回答这样安全，保险，可维护性强
  - 还有问我实习的细节？问我做的东西有意义嘛。。。

#### 手撕代码

- https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/

  - 我说先用暴力写，他说你不能用更好的方式，比如记录一下最小值？

  - ```java
        public int maxProfit(int[] prices) {
            int ans = 0;
            int length = prices.length;
            if(length == 1) return ans;
            int low = prices[0];
            for(int i = 1;i < prices.length;i++){
                if (prices[i] < low){
                    low = prices[i];
                }
                ans = Math.max(ans, prices[i] - low);
            }
            return ans;
        }
    ```

    

