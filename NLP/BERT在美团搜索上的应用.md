## BERT在美团搜索上的应用

[链接](https://tech.meituan.com/2020/07/09/bert-in-meituan-search.html)

#### 搜索相关性

- 给定用户的query 和候选的doc ,判断两者的相关性,相关性搞得排在前面,能够提高用户搜索效率和体验.

##### 字面相关性

- 字面匹配度: 字面命中,覆盖程度,TFIDF,BM25等
- 缺点: 
  - 缺少语义信息, 无法处理同义词和多义词
  - 缺少结构信息, "蛋糕奶油"和"奶油蛋糕"代表不同的需求

##### 语义相关性

- 隐式模型: 将Query和Doc映射到一个向量空间, 通过向量相似度来计算相关性, 可以通过LDA映射到同一向量空间
- 翻译模型: 通过统计机器翻译方法将Doc进行改写来和query匹配

##### 深度学习时代

##### 	基于表示的匹配	

-  双塔构想: 左塔输入query信息, 右塔输入Doc和Doc相关的信息,然后生成Query和Doc的高阶文本相关性,应用与排序模型上 

  ##### 基于交互的匹配

- 在神经网络底层就让Query和Doc提前交互，从而获得更好的文本向量表示，最后通过一个MLP网络获得语义匹配分数。

**基于表示的匹配方法优势在于Doc的语义向量可以离线预先计算，在线预测时只需要重新计算Query的语义向量，缺点是模型学习时Query和Doc两者没有任何交互，不能充分利用Query和Doc的细粒度匹配信号。基于交互的匹配方法优势在于Query和Doc在模型训练时能够进行充分的交互匹配，语义匹配效果好，缺点是部署上线成本较高。**

#### BERT语义相关性

- Feature-based: 类似DSSM模型, 基于表示的语义匹配, 编码成向量
- Finetune-based: 将query和doc输入句间关系fune-tuning ,通过MLP网络得到分数 

- 在美团的搜索业务中,整体上是一个二阶段模型, 通过基于BERT预测得到Query-Doc的相关性分数,输入核心排序模型(L2)进行排序

#### 算法探索

##### 数据增强

- 直接使用用户行为数据作为弱监督训练数据, 在DSSM模型中,每个query抽取一个正样本和四个负样本(query下被点击就认为是相关,从未被点击就认为是不相关)

###### 样本去噪

- 以商家(POI)搜索场景为例
  - 单字query表达的语义不完整,用户点击行为很随机,可以去除
  - 要过滤掉Query只出现在POI中分店的样本, 如"<大润发，小龙坎老火锅（大润发店）>"
- 负样本采样
  - 全局随机负采样, 用户没有点击的POI进行随机采样,但有业务过滤规则
    - 大量的POI未被用户点击是因为不是离用户最近的分店，但POI和Query是相关的，这种类型的样例需要过滤掉，如<蛙小侠 ，蛙小侠（新北万达店）>
    - 用户Query里包含品牌词，并且POI完全等同于品牌词的，需要从负样本中过滤，如<德克士吃饭 ，德克士>。
  - Skip-Above负采样
    - 并非所有POI被曝光过,未曝光的Doc直接作为负样本会有偏差, Skip-Above是从用户点击过的POI之上没有被点击的POI作为负样本

###### 品牌优化

- 保证同一品牌的的不同分店给出的相关性分数是一致的,不能影响排序模型计算
- 在品牌搜Query不包含地标词的时候，将POI名映射到品牌（非品牌POI不进行映射），从而消除品牌POI分店名中地标词引入的噪声。如Query是“香格里拉酒店”，召回的“香格里拉大酒店”和“北京香格里拉饭店”统一映射为品牌名“香格里拉酒店”。Query是“品牌+地标”形式（如“香格里拉饭店 北京”）时，用户意图明确就是找某个地点的POI，不需要进行映射

![img](https://p0.meituan.net/travelcube/66afcacfef98743952fa8befa82695c8140125.png)

##### 模型优化

##### 知识融合

- 通过构建的知识图谱(餐饮娱乐,还是比较偏垂类), 可以获得Doc的大量结构化信息,如地址,品类,场景等,通过这些结构化信息可以增强Doc的语义

- 模型输入
  - 将Query, Doc标题, 结构化信息拼接,用[SEP]分割. 原始的BERT只有$E_A$和$E_B$两种片段编码, 这里对结构化信息编码为$E_C$
  - ![img](https://p0.meituan.net/travelcube/af2a6102e46958b4301770dcb47a560643892.png)
- 模型训练目标
  - 去除NSP目标,改为判断用户是否点击 ,来进行fine-tuning

##### 引入实体成分识别的多任务Fine-tuning

- 引入实体识别任务进行fine-tuning

- ![img](https://p1.meituan.net/travelcube/18a1eab0d9b87abb38582ec5de38f743244650.png)

##### Pairwise Fine-tuning

- 上文的两个模型本质是Pointwise训练方式,能够学习到很好的全局相关性,但是忽略了不同样本间的偏序关系
- Pairwise 输入的是一个三元组, 同一个query下选择一个正例和一个负例组合成三元组作为输入样本,进行fine-tuning
- ![img](https://p1.meituan.net/travelcube/1aabd116bab390b493bdfdddb1fa7306168547.png)

##### 联合训练

- 前文的模型都是两阶段模型, 先训练BERT在训练L2排序, 为了让两者更好的融合, 尝试直接进行端到端训练
- ![img](https://p0.meituan.net/travelcube/55041ac3629136a0876291e43545ecd072335.png)

- **输入层**：模型输入是由文本特征向量、用户行为序列特征向量和其他特征向量3部分组成。
- 文本特征向量使用BERT进行抽取，文本特征主要包括Query和POI相关的一些文本（POI名称、品类名称、品牌名称等）。将文本特征送入预训练好的MT-BERT模型，取CLS向量作为文本特征的语义表示。
- 用户行为序列特征向量使用Transformer进行抽取[3]。
- 其他特征主要包括：① 统计类特征，包含Query、Doc等维度的特征以及它们之间的交叉特征，使用这些特征主要是为了丰富Query和Doc的表示，更好地辅助相关性任务训练。② 文本特征，这部分的特征同1中的文本特征，但是使用方式不同，直接将文本分词后做Embedding，端到端的学习文本语义表征。③ 传统的文本相关性特征，包括Query和Doc的字面命中、覆盖程度、BM25等特征，虽然语义相关性具有较好的作用，但字面相关性仍然是一个不可或缺的模块，它起到信息补充的作用。
- **共享层**：底层网络参数是所有场景网络共享。
- **场景层**：根据业务场景进行划分，每个业务场景单独设计网络结构，打分时只经过所在场景的那一路。
- **损失函数**：搜索业务更关心排在页面头部结果的好坏，将更相关的结果排到头部，用户会获得更好的体验，因此选用优化NDCG的Lambda Loss。

#### 应用实践

##### 模型轻量化

- BERT模型参数量过大, 前向计算耗时,需要进行轻量化
- 知识蒸馏:DistilBERT, TinyBERT
- 模型裁剪:使用模型剪枝减少参数规模
- 低精度量化: 在模型训练和推理中使用低精度（FP16甚至INT8、二值网络）表示取代原有精度（FP32）表示。???

##### 轻量化应用

- **query意图识别模型**: query通常很短, 裁剪几层Transformer 效果依旧很好
- **搜索场景**:query+doc拼接后文本很长,语义关系复杂,直接裁剪可能带来较大的性能损失. 采用**知识蒸馏**

#### 线上结果

- ![img](https://p0.meituan.net/travelcube/989237423ad321ef198c6e54078b4f4224237.png)

- 用户行为数据存在大量噪声不能直接拿来建模
- 知识融合的BERT模型引入大量结构化文本信息，弥补了POI名本身文本信息少的问题，排序模型CTR和NDCG都有明显的提升
- 模型损失函数改用排序任务常用的Pairwise Loss，其考虑了文档之间的关系更加贴合排序任务场景，线上排序模型NDCG取得了一定的提升