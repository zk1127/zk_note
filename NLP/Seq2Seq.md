



### Seq2Seq 

##### Seq2Seq在编码解码过程是否存在信息丢失？有哪些解决方案？

- 由于输入序列长度的增加，编码和解码过程的梯度消失和梯度爆炸现象变得更加严重
- 由于只是用固定大小的状态向量来连接编码模块和解码模块，要求解码器将整个输入序列的信息压缩到状态向量中，这个过程是一个有损压缩的过程

- 解决方案：
  - 机器翻译任务中有人将句子颠倒输入到编码器中，使得状态向量较好的保存原句靠前的单词
  - 注意力机制

#### 最初的Attention是在Seq2Seq模型，怎么计算的？

- 在解码器的每一步，使用 与编码器的直接连接 来专注于源序列的特定部分
- 包括点积注意力, 权重注意力, 拼接注意力(权重注意力效果最佳)
- 最后使用softmax进行归一化