## 集成学习-刘建平

### 

- boosting 算法需要解决的四个问题
  - 如何计算学习误差率e?
  - 如何得到弱学习器的权重系数$\alpha$?
  - 如何更新样本权重D?
  - 使用何种结合策略?

### Adaboost

- Adaboost损失函数为**指数函数**

- 假设训练样本为$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$
- 训练集在第K个弱学习器的输出权重:$D(k) = (w_{k1}, w_{k2}, ...w_{km}) ;\;\; w_{1i}=\frac{1}{m};\;\; i =1,2...m$

#### 二分类问题

##### 如何计算学习误差率e?

- $e_k=P(G_k(x_i) \ne y_i)=\sum_{i=1}^m w_{ki}I(G_k(x_i)\ne y_i)$

##### 如何得到弱学习器的权重系数$\alpha$?

- 如果误差率大,弱学习器的权重将减少,否则将增大
- $\alpha_k=1/2 log \cfrac{1-e_k}{e_k}$
- 对于多分类问题$\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k} + log(R-1)$

##### 如何更新样本权重D?

- 在第K轮,样本在第K+1个弱分类器中权重增大,如果分类错误,权重将减少
- $w_{k+1,i} = \frac{w_{ki}}{Z_K}exp(-\alpha_ky_iG_k(x_i))$
- 其中 $Z_k = \sum\limits_{i=1}^{m}w_{ki}exp(-\alpha_ky_iG_k(x_i))$

##### 使用何种结合策略?

- 加权表决法
- $f(x) = sign(\sum\limits_{k=1}^{K}\alpha_kG_k(x))$

#### 回归问题

- Adaboost R2算法为例

##### 如何计算学习误差率e?

- 在第k个弱分类器中计算最大误差:$E_k=max|y_i-G_k(x_i)|$
- 然后计算每个样本的相对误差:$e_{ki}=\cfrac{|y_i-G_k(x_i)|}{E_k}$
  - 相对误差也可以计算为:$e_{ki}=\cfrac{(y_i-G_k(x_i))^2}{E_k}$或者$e_{ki}=1-exp(\cfrac{-y_i+G_k(x_i)}{E_k})$
- 所以误差率可以计算为:$e_k =  \sum\limits_{i=1}^{m}w_{ki}e_{ki}$

##### 如何得到弱学习器的权重系数$\alpha$?

- $\alpha_k=\cfrac{e_k}{1-e_k}$

##### 如何更新样本权重D?

- $w_{k+1,i} = \frac{w_{ki}}{Z_k}\alpha_k^{1-e_{ki}}$
- $Z_k = \sum\limits_{i=1}^{m}w_{ki}\alpha_k^{1-e_{ki}}$

##### 使用何种结合策略?

- 采用的是对加权的弱学习器取权重中位数对应的弱学习器作为强学习器的方法

#### Adaboost优缺点

- 优点
  - 效果好,构建简单
  - 不容易过拟合

- 缺点
  - 对异常样本敏感

### GBDT

#### 核心思想

- 使用损失函数的负梯度来拟合本轮损失的近似值
- 使用CART回归树模型作为弱学习器

#### 负梯度拟合

- 负梯度计算: $r_{ti}=-[\cfrac{\part L(y_i,f(x_i))}{\part f(x_i)}]_{f(x)=f_{t-1}(x)}$
- **利用$(x_i,r_{ti})$可以拟合一颗CART回归树**,得到第t颗回归树,其对应的叶节点区域$R_{tj},j = 1,2,...,J$,其中J作为叶子节点的个数
  - $c_{tj}=argmin_c \sum_{x_i\in R_{tj}}L(y_i,f_{t-1}(x_i) + c)$
- 本轮决策的拟合函数为:
  - $h_t(x)=\sum_{j=1}^J c_{tj}I(c\in R_{tj})$
- 最终得到的强学习器的表达式为:
  - $f_t(x)=f_{t-1}(x)+\sum_{j=1}^J c_{tj} I(x \in R_{tj})$

- GBDT 的loss函数
  - 回归问题使用均方差loss,绝对误差loss,Huber损失
  - 分类问题使用指数loss ,$exp(-yf(x))$或者对数似然损失

##### GBDT回归算法

- 输入为样本:$T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$, 最大迭代次数T, 损失函数L
- 初始化弱学习器
  - $f_0(x)=argmin_c \sum_{i-1}^m L(y_i,c)$

- 迭代
  - 计算负梯度:$r_{ti} = -\bigg[\frac{\partial L(y_i, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)}$
  - 计算拟合值:$c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} L(y_i,f_{t-1}(x_i) +c)$
  - 更新强学习器: $f_{t}(x) = f_{t-1}(x) + \sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$
  - 最终强学习器表示:$f(x) = f_T(x) =f_0(x) + \sum\limits_{t=1}^{T}\sum\limits_{j=1}^{J}c_{tj}I(x \in R_{tj})$

##### GBDT分类算法(对数似然损失)

- 损失函数定义: $L(y, f(x)) = log(1+ exp(-yf(x)))$
- 负梯度定义:$r_{ti} = -\bigg[\frac{\partial L(y, f(x_i)))}{\partial f(x_i)}\bigg]_{f(x) = f_{t-1}\;\; (x)} = y_i/(1+exp(y_if(x_i)))$
- 求每个叶子节点的最佳负梯度拟合值为: $c_{tj} = \underbrace{arg\; min}_{c}\sum\limits_{x_i \in R_{tj}} log(1+exp(-y_i(f_{t-1}(x_i) +c)))$
  - 上式难以优化,需要用$c_{tj} = \sum\limits_{x_i \in R_{tj}}r_{ti}\bigg / \sum\limits_{x_i \in R_{tj}}|r_{ti}|(1-|r_{ti}|)$

##### GBDT正则化

- 设置步长:$f_{k}(x) = f_{k-1}(x) + \nu h_k(x)$
- 子采样
- 树剪枝

#### GBDT小结

- 优点:
  - 性能好
- 缺点:
  - 难以并行训练数据

### XGBoost

#### 从GBDT到XGBoost

- 算法本身的优化: XGBoost支持多种软学习器, XGBoost对误差函数进行二阶泰勒展开
- 算法运行效率的优化: 先对所有的特征的值进行排序分组，方便前面说的并行选择,对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。
- 算法健壮性的优化: 对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。

##### XGBoost的损失函数

- $L_t=\sum\limits_{i=1}^mL(y_i, f_{t-1}(x_i)+ h_t(x_i)) + \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2$

- 正则化项:

  - $\Omega(h_t) = \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2$

  - J表示叶子节点的个数, **$w_{tj}$是第j个节点的最优值**

- 由于XGBoost会使用二阶泰勒展开作为负梯度

  - $$
    \begin{align} L_t & = \sum\limits_{i=1}^mL(y_i, f_{t-1}(x_i)+ h_t(x_i)) + \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2 \\ & \approx  \sum\limits_{i=1}^m( L(y_i, f_{t-1}(x_i)) + \frac{\partial L(y_i, f_{t-1}(x_i) }{\partial f_{t-1}(x_i)}h_t(x_i) + \frac{1}{2}\frac{\partial^2 L(y_i, f_{t-1}(x_i) }{\partial f_{t-1}^2(x_i)} h_t^2(x_i)) + \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2  \end{align}
    $$

-  这里为了简便,把第$i$个样本在第t个弱学习器的一阶和二阶导数分别计为:

  - $$
    g_{ti} = \frac{\partial L(y_i, f_{t-1}(x_i) }{\partial f_{t-1}(x_i)}, \; h_{ti} = \frac{\partial^2 L(y_i, f_{t-1}(x_i) }{\partial f_{t-1}^2(x_i)}
    $$

  - 这时损失函数为: $L_t \approx \sum\limits_{i=1}^m( L(y_i, f_{t-1}(x_i)) + g_{ti}h_t(x_i) + \frac{1}{2} h_{ti} h_t^2(x_i)) + \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2$

**要极小化上面这个损失函数，得到第t个决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解$w_{tj}$**

- 由于损失函数中$L(y_i, f_{t-1}(x_i))$对于$h_t$是常数项,对最小化没有影响, 损失函数可以继续简化,同时由于每个决策树的第j个叶子节点的取值最终会是同一个值$w_{tj}$,因此我们的损失函数可以继续化简。

  - $$
    \begin{align} L_t & \approx \sum\limits_{i=1}^m g_{ti}h_t(x_i) + \frac{1}{2} h_{ti} h_t^2(x_i)) +  \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2  \\ & = \sum\limits_{j=1}^J (\sum\limits_{x_i \in R_{tj}}g_{ti}w_{tj} +  \frac{1}{2} \sum\limits_{x_i \in R_{tj}}h_{ti} w_{tj}^2) +  \gamma J + \frac{\lambda}{2}\sum\limits_{j=1}^Jw_{tj}^2 \\ & =  \sum\limits_{j=1}^J [(\sum\limits_{x_i \in R_{tj}}g_{ti})w_{tj} + \frac{1}{2}( \sum\limits_{x_i \in R_{tj}}h_{ti}+ \lambda) w_{tj}^2] + \gamma J   \end{align}
    $$

- 把每个叶子节点区域样本的一阶和二阶导数的和单独表示如下：

  - $$
    G_{tj} = \sum\limits_{x_i \in R_{tj}}g_{ti},\; H_{tj} =  \sum\limits_{x_i \in R_{tj}}h_{ti}
    $$

- 最终损失函数形式

  - $$
    L_t  =  \sum\limits_{j=1}^J [G_{tj}w_{tj} + \frac{1}{2}(H_{tj}+\lambda)w_{tj}^2] + \gamma J
    $$

    

##### XGBoost损失函数的优化求解

- 如何求出每个叶子节点区域的最优解$w_{tj}$？
  - 直接求导令导数为0 , 可得: $w_{tj} = - \frac{G_{tj}}{H_{tj} + \lambda}$

- 如何选择哪个特征和特征值进行分裂，使最终我们的损失函数$L_t$最小？
  - 在GBDT里面，我们是直接拟合的CART回归树，所以树节点分裂使用的是均方误差。XGBoost这里不使用均方误差，而是使用贪心法，即每次分裂都期望最小化我们的损失函数的误差。
  - 带入最优解可得:
    - $L_t=-1/2\sum_{j=1}^J \cfrac{G^2_{tj}}{H_{tj}+\lambda} + \gamma J$
- 如果我们每次做左右子树分裂时，可以最大程度的减少**损失函数的损失**(分裂增益)就最好了。也就是说，假设当前节点左右子树的一阶二阶导数和为$G_L$,$H_L$,$G_R$,$H_R$ 则我们期望最大化下式:
  - $-\frac{1}{2}\frac{(G_L+G_R)^2}{H_L+H_R+ \lambda} +\gamma J -(  -\frac{1}{2}\frac{G_L^2}{H_L + \lambda}  -\frac{1}{2}\frac{G_{R}^2}{H_{R} + \lambda}+ \gamma (J+1) )$
  - 即$\max \frac{1}{2}\frac{G_L^2}{H_L + \lambda} + \frac{1}{2}\frac{G_R^2}{H_R+\lambda} - \frac{1}{2}\frac{(G_L+G_R)^2}{H_L+H_R+ \lambda} - \gamma$

#### XGBoost算法流程

- 输入是训练集样本$I=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$， 最大迭代次数T, 损失函数L， 正则化系数$\lambda, \gamma$。

- 　对迭代轮数t=1,2,...T有：

  1. 计算第i个样本(i-1,2,..m)在当前轮损失函数L基于$f_{t-1}(x_i)$的一阶导数$g_{ti}$和二阶导数$h_{ti}$,计算所有样本的一阶导数和$G_t = \sum\limits_{i=1}^mg_{ti}$, 二阶导数和$H_t = \sum\limits_{i=1}^mh_{ti}$

  2. 基于当前节点尝试分裂决策树，默认分数score=0，G和H为当前需要分裂的节点的一阶二阶导数之和。

  - 对特征序号 k=1,2...K:

    a). 令$G_L=0, H_L=0$

    b.1) 将样本按特征k从小到大排列，依次取出第i个样本，依次计算当前样本放入左子树后，左右子树一阶和二阶导数和：

    - $$
      G_L = G_L+ g_{ti}, G_R=G-G_L\\
      H_L = H_L+ h_{ti}, H_R=H-H_L
      $$

    - 

    b.2)尝试更新最大的分数：

    - $$
      score = max(score, \frac{1}{2}\frac{G_L^2}{H_L + \lambda} + \frac{1}{2}\frac{G_R^2}{H_R+\lambda}  - \frac{1}{2}\frac{(G_L+G_R)^2}{H_L+H_R+ \lambda} -\gamma )
      $$

      

  3. 基于最大score对应的划分特征和特征值分裂子树。

  4. 如果最大score为0，则当前决策树建立完毕，计算所有叶子区域的$w_{tj}$,得到弱学习器$h_t(x)$, 更新强学习器$f_t(x)$, 进入下一轮弱学习器迭代.如果最大score不是0，则转到第2)步继续尝试分裂决策树。

#### XGBoost算法运行效率的优化

- 对训练的每个特征排序并且以块的的结构存储在内存中，方便后面迭代重复使用，减少计算量。计算量的减少参见上面第4节的算法流程，首先默认所有的样本都在右子树，然后从小到大迭代，依次放入左子树，并寻找最优的分裂点。这样做可以减少很多不必要的比较。

#### XGBoost算法健壮性的优化

- 最后我们再来看看XGBoost在算法健壮性的优化，除了上面讲到的正则化项提高算法的泛化能力外，XGBoost还对特征的缺失值做了处理。
- XGBoost没有假设缺失值一定进入左子树还是右子树，则是尝试通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向，这样处理起来更加的灵活和合理。
- 也就是说，上面第4节的算法的步骤a),b.1)和b.2)会执行2次，第一次假设特征k所有有缺失值的样本都走左子树，第二次假设特征k所有缺失值的样本都走右子树。然后每次都是针对没有缺失值的特征k的样本走上述流程，而不是所有的的样本
- 如果是所有的缺失值走右子树，使用上面第4节的a),b.1)和b.2)即可。如果是所有的样本走左子树，则上面第4节的a)步要变成：$G_R=0, H_R=0$

- 　b.1)步要更新为：

- $$
  G_R = G_R+g_{ti}, G_L=G-G_R\\
  H_R = H_R+h_{ti}, H_L=H-H_R
  $$

  