

### 损失函数

##### 目标

- 用来评估模型的预测值和真实值不一样的程度，损失函数越好，模型的性能越好

##### 分类

- 经验损失函数：预测结果与真实结果的差别
- 结构损失函数：经验损失函数加上**正则项**

#### 常见损失函数

##### 0-1损失函数

- 预测值与目标值不相等时为1,否则为0
- $L(Y,f(X)) = \{1,Y \neq f(X); 0, Y = f(X)\}$

- 特点：

  - 直接定义分类错误的个数，是一个非凸的函数，不太使用

  - **感知机**使用这种损失函数，适当放宽条件可以修改为

    $$L(Y,f(X)) = \{1,|Y-f(X)| \geq T; 0, |Y-f(X)| < T\}$$

##### 绝对值损失函数

- 计算预测值与目标值的差的绝对值
- $$L(Y,f(X))=|Y-f(x)|$$

##### log对数损失函数

- $L(Y,p(Y|X)) = -log(Y|X)$
- 特点：
  - 能非常好的表征概率分布，在很多场景适用于多分类
  - 健壮性不强，相比于hinge loss对噪声非常敏感
  - **逻辑回归**的损失函数就是log对数损失函数

##### 平方损失函数

- $L(Y|f(x))=\sum_N(Y - f(x))^2$

- 特点
  - 经常应用于**回归**问题

##### 指数损失函数(exponential loss)

- $L(Y|f(X))=exp(-yf(x))$
- 特点
  - 对离群点非常敏感，经常应用在AdaBoost算法上

##### Hinge损失函数

- $L(Y,f(x)) = max(0,1-yf(x))$
- 特点
  - 如果分类正确则损失为0，分类错误则损失为$1-yf(x)$, **SVM**所使用的损失函数
  - 一般$f(x)$是预测值，在-1到1之间，y是目标值(-1,1)。其含义是$f(x)$的值是在-1和1之间就可以了，并不鼓励$|f(x)|>1$,不鼓励分类去过度自信，从而是分类器**更加关注整体的误差**
  - 健壮性相对较高，对异常点、噪声不敏感，但它没有很好的概率解释

##### 感知损失函数

- $L(Y,f(X))=max(0,-f(x))$
- 特点
  - 是Hinge损失函数的一个变种，perceptron loss只要样本的判定类别正确的话，它就满意，不管其判定边界的距离。
  - 比Hinge loss简单，但是模型泛化能力没有Hinge loss强

##### 交叉熵损失函数

- $C = \cfrac{1}{n}\sum_x[ylna +(1-y)ln(1-a)]$

- x表示样本，y表示实际的标签，a表示预测的输出，n表示样本总数量
- 特点：
  - 本质是一种**对数似然函数**，可以用于**二分类和多分类**
  - 二分类的loss函数(输入数据是softmax或者sigmoid函数的输出)
    - $loss = -\cfrac{1}{n}\sum_x[ylna +(1-y)ln(1-a)]$
  - 多分类的loss函数(输入数据是softmax或者sigmoid函数的输出)
    - $loss = -\cfrac{1}{n}\sum_iy_iln a_i$

- 使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**，因为它可以**完美解决平方损失函数权重更新过慢**的问题，
- 具**误差大的时候，权重更新快；误差小的时候，权重更新慢**的良好性质。

##### 1.**交叉熵函数**与**最大似然函数**的联系和区别？

区别：交叉熵函数用来描述模型预测值和真实值的差距大小，越大代表越不相近，似然函数的本质是衡量在某个参数下，整体的估计和真实情况一样的概率，越大代表越相近

联系：**交叉熵函数**可以由最大似然函数在伯努利分布的条件下推导出来，或者说**最小化交叉熵函数**的本质就是**对数似然函数的最大化**。

##### 2 如何防止过拟合

- 获取更多的数据，数据增强
- 深度学习的话，减少模型层数，神经元个数
- 正则化
- 增加噪声，在输入中加入高斯噪声与L1, L2正则化的效果类似
- Dropout

