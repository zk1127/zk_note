

### 决策树

##### 1. 为什么集成学习会使用决策树模型作为基学习器？

- 决策树模型的表达和泛化能力可以通过调节树的层数来折中
- 样本的扰动对决策树的影响较大，树形模型由于结构简单能够较好的引入随机性
- 除了决策树之外，神经网络模型也比较不稳定，可以通过调整神经元数量，层数，初始权重引入随机性

##### 2. ID3, C4.5, CART 这三种决策树的区别

- ID3基于信息增益，C4.5基于信息增益率，CART基于基尼系数和最小平方误差作为特征选择的标准
- ID3：
  - 信息熵：$H(D) = -\sum_i \cfrac{|C_k|}{C}log(\cfrac{|C_k|}{C})$ 
  - 条件熵：$H(D|A)=-\sum_i \cfrac{|D_i|}{D}H(D_i)$
  - 信息增益：$g(D,A) = H(D) - H(D|A)$
- C 4.5:  基于信息增益率可以避免ID3偏向选择多值属性的不足
  - 信息增益比： $g_R(D,A) = \cfrac{g(D,A)}{H(D)}$
- CART：既可以用于分类又可以用于回归树：
  - $Gini(D) = \sum_i^{|K|}p_k(1-p_k) = 1 - \sum_i^{|K|} p_k^2$, 基尼系数越小越好

##### 决策树的剪枝？

- 剪枝是决策树非常重要的步骤，由于决策树的深度不断扩展，很容易出现过拟合，必须进行剪枝提高泛化能力
- 预剪枝：在决策树生成过程中，使用验证集估计其划分后的性能，如果不能提升就立即停止划分
- 后剪枝：在决策树构建完成后，自下而上地对决策树进行考察，若将此节点标记为叶子节点可以带来更好的泛化性能，则修改之。







### RandomForest、GBDT、XGBoost、lightGBM

*四者都属于集成学习模型，集成学习是结合多个基本学习器的预测结果来改善基本学习器的泛化能力和鲁棒性*

##### 集成学习的步骤

- 找到误差互相独立的基分类器
- 训练基分类器
- 合并基分类器的结果
  - voting和stacking两种，前者用投票的方式将获得最多选票的结果作为最终的结果，后者用串行的方式将前一个分类器的结果输出到下一个分类器，然后将所有分类器的结果相加或者融合（比如把所有输出作为特征输入到逻辑回归中进行最后结果的预测）。

**以AdaBoost为例**

 - 确定以ID3决策树为基学习器
 - 训练基分类器
    - 假设有T个分类器，N个训练样本，初始化采样分布$D_1(i)=1/N$
    - 从1-T的循环，从训练集中采样$S_t$
    - 计算$h_t$的错误率：$\epsilon_t=\cfrac{\sum_{i=1}^{N_t}I[h_t(x_i)\neq y_i]D_t(x_i)}{N_t}$
    - 计算基分类器$h_t$权重$a_t=log \cfrac{(1-\epsilon_t)}{\epsilon_t}$
    - 设置下一次采样
       - 当$h_t(x_i) \neq y_i$,$D_{t+1}=D_t(i) \; or \; D_t(i)(1-\epsilon_t)/\epsilon_t$
       - 当$h_t(x_i) = y_i$,$D_{t+1}=D_t(i)\epsilon_t/(1-\epsilon_t)$
 - 合并基分类器，给定一个未知样本z，输出结果为加权投票的结果$Sign(\sum_{t=1}^Th_t(z)a_t)$

##### 集成学习的种类

- Bagging
  - 并行训练的基分类器
  - 随机**可放回抽样**，k个训练集
  - 投票决策(分类)或者平均加权(回归)
  - 泛化能力强，**降低方差**, 上限低
- Boosting
  - 串行训练的分类器
  - 每一次训练给前一次训练分类错误的**样本加权**
  - 性能优越, **降低偏差**,噪声敏感

##### Bagging 和Boosting的区别？

- Bagging对样本进行抽样选择，Boosting是对所有样本进行学习
- Bagging可以并行加速训练，Boosting只能串行训练
- Boosting 会给样本和分类器赋予不同的权重，Bagging平等看待所有样本和分类器

##### RandomForest

- 是Bagging的扩展体，Bagging可以理解为：放回抽样，多数表决(分类)和简单平均(回归)，Bagging多个学习器直接属于并列生成，不存在强依赖关系
- Random Forest是在Bagging的基础上，引入**决策树**作为基本学习器，进一步在决策树的训练过程中引入随机特征选择
- 随机性体现在**数据采样的随机性与特征采用的随机性**

- 过程
  - 随机样本选择（放回抽样）
  - 随机特征属性选择(避免学习器之间过强的相关性)
  - 构建决策树
  - 随机森林投票，因此防止过拟合能力强，降低方差
- 优点
  - 解决分类与回归两种类型的问题，表现良好，由于是集成学习，**方差和偏差都比较低**，**泛化性能优越**
  - 可以并行训练，训练效率高
  - 可以应对缺失数据
  - 对于高维数据处理效果好，能够处理成千上万的输入变量，并确定最重要的变量，可以被应用于降维任务
- 缺点
  - 在回归问题上表现的不如分类问题，由于其不能作出训练集以外的预测，可能导致过拟合
  - 过于黑盒
  - 忽略了参数之间的相关性

##### 可否将随机森林中的基分类器,由决策树替换为线性分类器或K-近邻?请解释为什么?

- 不可，随机森林是一种bagging类的算法，他的好处是可以降低方差，所以它希望基学习器是对样本分布敏感的学习器，这样bagging才能有用武之地。线性分类器或K-近邻都是非常稳定的分类器，本身方差就不大
- 而且由于随机森林的随机抽样，可能会导致线性分类器或K-近邻很难收敛，增大集成学习器的偏差

##### GBDT（Gradient Boosting Decision Tree）

- Boosting 是序列化训练多轮学习器，将弱学习器提升为强学习器，增大前一轮被分错样本的权值，减少被分对样本的权值，加权求和学出分类器
- GBDT原理
  - 是boost里面的提升树，并使用gradient boost
  - GradientBoosting 算法的关键是利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一颗CART回归树, 每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，**在Gradient Boosting中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少**
  - 与Boosting Tree的区别：Boosting Tree适合于损失函数为**平方损失或者指数损失**。而Gradient Boosting适合各类损失函数（损失函数为平方损失则相当于Boosting Tree拟合残差、损失函数为指数损失则可以近似于**Adaboost**，但树是回归树）
  - GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树) 因为Gradient Boosting 需要按照**损失函数的梯度**近似的拟合残差，这样拟合的是连续数值，因此只有回归树。
- 优点
  - 在分布稠密的数据集上泛化能力和表达能力很好
  - 使用决策树模型使得GBDT具有较好的可解释性和鲁棒性
- 在较小的调参时间也能有很好的预测性能
  
- 缺点
  - 在高维稀疏的数据上性能不如神经网络和SVM
  - 难以并行训练

##### 梯度提升和梯度下降的区别和联系是什么?

- 梯度下降和梯度提升都是在每一轮迭代过程中，利用损失函数相对于模型的负梯度方向的信息来进行更新
- 梯度下降本质是是对参数进行更新，梯度提升中是直接定义在函数空间，从而大大提升了可以使用模型的种类

##### XGBoost与GBDT的联系和区别有哪些?

- GBDT是机器学习算法，XGBoost是该算法的工程实现
- 在使用CART作为基分类器时，XGBoost显示的加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力，*GBDT是在决策树构建后进行剪枝*
- GBDT在模型训练过程中只是用代价函数的一阶导数信息，XGBoost对代价函数进行了二阶泰勒展开，可以同时使用一阶和二阶导数信息
- 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器
- 传统的GBDT每轮迭代都使用了全部数据，XGBoost采用随机森林类似的策略，支持对数据进行采样
- 传统的GBDT没有对缺失值进行处理，XGBoost能够自动学习缺失值的处理策略

##### XGBoost

- GBDT是用模型在数据上的负梯度作为残差的近似值，从而拟合残差。XGBoost也是拟合的在数据上的残差，但是它是用**泰勒展开式**对模型损失残差的近似；同时XGBoost对模型的损失函数进行的改进，并加入了模型复杂度的**正则项**。
- 性能进一步提高，能够并行运算
- 与GBDT的区别
  - 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）;
  - 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，**得到一阶和二阶导数**；
  - XGBoost在代价函数中加入了**正则项**，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性；
- 优点
  - shrinkage（缩减），相当于**学习速率**（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；
  - 列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算；
  - 对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动学习出它的分裂方向；
  - XGBoost支持并行: 仍然是Boosting 模型，并行在于特征粒度上，决策树一个耗时步骤是对特征值进行排序，XGBoost是训练之前预先对数据进行排序，然后保存为block结构，后面迭代重复利用这个结构，从而减少计算量。

- 缺点
  - level-wise 建树方式对当前层的所有叶子节点一视同仁，有些叶子节点分裂收益非常小，对结果没影响，但还是要分裂，加重了计算代价。
  - 预排序方法空间消耗比较大，不仅要保存特征值，也要保存特征的排序索引，同时时间消耗也大，在遍历每个分裂点时都要计算分裂增益(不过这个缺点可以被近似算法所克服)

##### lightGBM

- 基本原理与XGBoost一样，只是在框架上做了一优化
- 直方图算法
- 特征合并
- 单边抽样
- xgboost是level-wise对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了不必要的开销。 lightGBM采用leaf-wise的做法，在当前所有叶子节点中选择分裂**收益最大的节点**进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。

