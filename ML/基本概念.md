

### 基本概念



#### 熵

- 定义：随机变量分布的混乱程度，分布越混乱，熵越大

##### 自信息

- 定义：表示某一事件带来的信息量的大小

- 理解：当事件发生的概率越大，自信息越小（某一事件发生的概率非常大，并且实际上也发生了，则此时的自信息较小。），反之则自信息越大

  *一个不太可能发生的事情，要比一件非常可能发生的事情提供更多的信息*

- 公式：

$$I(p_i)=-log(p_i)$$

- 与随机变量的取值无关，只与随机变量的概率有关
- 其中$p_i$表示随机变量的第i个事件发生的概率，自信息单位是bit, 表征描述该信息需要多少位。

##### 互信息

$$
I(X,Y)=\sum_{y \in Y}\sum_{x \in X} p(x,y)log\cfrac{p(x,y)}{p(x)p(y)}
$$

- 当X，Y相互独立时，$I(X,Y)$最小为0

##### 信息熵

- 定义：用来描述整个随机分布所带来的信息量平均值，**更具有统计特性**，所有事件信息量的**期望**
- 在机器学习中，由于熵的计算是依据样本数据而来，故也叫**经验熵**
- 公式：

$$H(X) = E_{x-p}[I(X)]=-E_{x-p}[log(p_i)]=-\sum_{i=1}^{n}p(x_i)log(p_i)$$

##### 条件熵

- 在X给定条件下，Y的条件概率分布的熵对X的数学期望。
- 公式

$$H(Y|X) = H(X,Y) - H(X)$$

##### 交叉熵

- 度量两个概率分布p和q间的差异性信息

$$H(p,q)=E_{x-p}[-logq(x)]=-\sum_{i=1}^np
(x)log(q(x))$$

- 信息论中，表示用编码方式q来编码真实分布p，需要消耗多少个bit数

##### 相对熵(KL散度)

- 经常应用于贝叶斯框架，$D_{KL}(p||q)$衡量了从先验分布q到后验分布的信念之后带来的信息增益

$$D_{KL}(p||q)=E_{x-p}[log \cfrac{p(x)}{q(x)}]=-E_{x-p}[log \cfrac{q(x)}{p(x)}]=-\sum_{i=1}^{n}p(x)log \cfrac{q(x)}{p(x)}=H(q,p)-H(p)$$

- 特征
  - 当p分布和q分布相等时候，KL散度值为0，这是一个非常好的性质；
  - 可以证明是非负的；
  - 非对称的，通过公式可以看出，KL散度是衡量两个分布的不相似性，不相似性越大，则值越大，当完全相同时，取值为0。
- **在最优化问题中，最小化相对熵等价于最小化交叉熵；相对熵和交叉熵的定义其实都可以从最大似然估计得到**
- 交叉熵广泛应用于Sigmoid 和Softmax 函数的损失
- 相对熵大量应用在生成模型中，例如GAN、EM、贝叶斯学习和变分推导中。

##### 交叉熵与KL散度的关系

- 如果只针对q(x)进行最小化KL散度变化，那么他与交叉熵等价

- 相对熵和交叉熵的定义其实都可以从最大似然估计得到**