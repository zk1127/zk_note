### 基本概念

##### 什么是Batch Norm,什么是Layer Norm?有何作用？

- Batch Norm,m 是batch size

$$
\mu_k = \cfrac{1}{m}\sum_{i=1}^m x_i\\
\sigma_k^2 = \cfrac{1}{m} \sum_{i=1}^m(x_i - \mu_k)^2\\
y^k=\gamma^k \cfrac{x^k - \mu^k}{\sqrt{(\sigma^k)^2 + \epsilon}} + \beta^k
$$

- Batch Norm的作用
  
  - 加快了梯度下降求解最优解的速度
  - 为了避免输入数据分布变化带来的不稳定现象
- 抑制过拟合
  - 他保留了神经网络各层在训练过程中的学习成果，如果没有$\beta$和$\gamma$，批归一化将退化为普通的标准化
  - 保证了激活单元的非线性表达能力
  
- Layer Norm

  - BN是在层内进行Normalization,LN直接对隐层单元的输出做Normalization ，最大的好处是不依赖batch size，H是隐层单元层数

  $$
  \mu_k = \cfrac{1}{H}\sum_{i=1}^H a_i\\
  \sigma_k^2 = \cfrac{1}{H} \sum_{i=1}^H(a_i - \mu_k)^2\\
  y^k=\gamma^k \cfrac{x^k - \mu^k}{\sqrt{(\sigma^k)^2 + \epsilon}} + \beta^k
  $$

  

##### 卷积网络的作用

- 局部连接和权值共享
- 局部连接是不同卷积关注不同局部的特征
- 局部不变性：图像上的平移不变性，旋转不变性，尺度不变性
- 组合性：基本特征可以组合为复杂特征

##### 为什么需要反向传播(BP)?

- 神经网络的目标是为了更好地拟合一个函数，学习的目的是为了需出最符合样本分布的参数，梯度下降法就是为了更好地求解最小化cost问题
- 神经网络有很多条前向计算路径，反向传播类似动态规划，从后往前进行传播可以保证只需要每条路径访问一次就能够求出顶点对所有下层节点的偏导值，从而减少冗余计算

##### 为什么不在NLP任务中使用Batch Normalization?

- 因为在每个batch中,文本长度不一样,实际上捕捉的均值和方差不一定有意义
- 第二个是因为batch normalization 对每个位置进行归一化,实际上与nlp任务本身不匹配,因为并非同一位置的单词就是同一种特征
- 而layer normalization 把整个文本看作一个层面特征,进行归一化,实际上就是类似词语向量加权表示句子.一个句子中的词的上下文是有关系的，做归一化有助于学习，快速收敛。

