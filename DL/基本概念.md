### 基本概念

##### 什么是Batch Norm,什么是Layer Norm?有何作用？

- Batch Norm,m 是batch size

$$
\mu_k = \cfrac{1}{m}\sum_{i=1}^m x_i\\
\sigma_k^2 = \cfrac{1}{m} \sum_{i=1}^m(x_i - \mu_k)^2\\
y^k=\gamma^k \cfrac{x^k - \mu^k}{\sqrt{(\sigma^k)^2 + \epsilon}} + \beta^k
$$

- Batch Norm的作用
  
  - 加快了梯度下降求解最优解的速度
  - 为了避免输入数据分布变化带来的不稳定现象
- 抑制过拟合
  - 他保留了神经网络各层在训练过程中的学习成果，如果没有$\beta$和$\gamma$，批归一化将退化为普通的标准化
  - 保证了激活单元的非线性表达能力
  
- Layer Norm

  - BN是在层内进行Normalization,LN直接对隐层单元的输出做Normalization ，最大的好处是不依赖batch size，H是隐层单元层数

  $$
  \mu_k = \cfrac{1}{H}\sum_{i=1}^H a_i\\
  \sigma_k^2 = \cfrac{1}{H} \sum_{i=1}^H(a_i - \mu_k)^2\\
  y^k=\gamma^k \cfrac{x^k - \mu^k}{\sqrt{(\sigma^k)^2 + \epsilon}} + \beta^k
  $$

  

##### 卷积网络的作用

- 局部连接和权值共享
- 局部连接是不同卷积关注不同局部的特征
- 局部不变性：图像上的平移不变性，旋转不变性，尺度不变性
- 组合性：基本特征可以组合为复杂特征



