### 图神经网络

### 基本介绍

##### 图神经网络的实用框架

$$
Y = \sigma(\hat D^{-1} \hat A XW)
$$

- X,Y表示输入和输出，$\hat A$表示加了自环的邻接矩阵，$\hat D$是$\hat A$的度矩阵，W表示待训练的参数矩阵
- XW将节点的特征向量进行线性变化
- $\hat A XW$将变化后的节点特征传播到邻居节点
- $\hat D^{-1} \hat A XW$将每个节点收到的特征进行归一化
- $\sigma(\hat D^{-1} \hat A XW)$将归一化的特征进行非线性激活单元

#### 图数据的特殊性质

- 节点的不均匀分布
- 排列不变性：任意变换图中的两个节点的空间位置，整个图结构是不变的
- 边的额外属性

#### 图神经网络的发展

- 早期不动点理论：使用RNNs通过邻居节点和边递归更新状态，知道达到不动点
- 谱域图神经网络：
  - 将图的拉普拉斯矩阵进行谱分解，并据此进行定义卷积运算，计算复杂度高，且无法扩展到其他图
  - ChebyNet: 使用切比雪夫展开来近似卷积核
  - GCN：简化切比雪夫网络，只使用一阶近似卷积核。实际上在GCN中，已经是一阶邻居节点间的信息传递
- 空域图神经网络：
  - GAT：利用注意力机制定义图神经网络
  - GraphSAGE: 将直推学习推广到归纳式学习，而且使用了采样的方式加速学习(归纳式的不需要测试数据出现在训练中，直推式的需要)

#### 图数据的问题

- 节点：节点分类，回归，聚类
- 边：边分类，链路预测(推荐)
- 图：图分类，生成，匹配
- NLP中的应用：序列标注，语法树，文本分类，知识图谱

### 模型

#### 谱域图神经网络

##### 最初形式

- $y=g_{\theta} * \mathcal{G}x=g_{\theta}(U\Lambda U^T)x=U g_{\theta} U^Tx$​
- U是拉普拉斯矩阵的特征向量,$U g_{\theta} U^T$​可以看作卷积核
- 计算量大，在空域上没有明确意义

##### 切比雪夫网络

- $y=g_{\theta} * \mathcal{G}x=U g_{\theta} U^Tx$​​
- 为了使得卷积函数具有更好的局部性，可以把g定义为一个拉普拉斯矩阵的函数$g_{\theta}(L)$​
- 切比雪夫网络将其近似为K阶切比雪夫多项式的近似,切比雪夫不等式可以递归求解
- $g_{\theta}(L)=\sum_{k=0}^K \theta_kT_k(\hat{L})$​​​​,且$\hat{L}=2L/\lambda_{max}-I_n$​
- 切比雪夫网络保留了K-局部化，节点受到周围K阶邻居影响

##### 图卷积网络

- 将切比雪夫网络多项式核定义为1阶，叠加K层这种卷积层，可以将节点的影响扩展到K阶，而且使得节点对K阶邻居的依赖变得更加弹性
- GCN的图卷积定义:$H^{l+1}=\sigma(\hat{A}H^lW^l)$,其中$\hat{A}=\hat{D}^{1/2}A\hat{D}^{1/2}$​
- 图卷积是一种特殊形式的拉普拉斯平滑（拉普拉斯平滑是为了让一个点和周围节点尽可能相似）
- 当卷积核超过两层，就会出现过平滑问题
- 改进
  - 图卷积网络模型和随机游走模型想结合
  - 残差连接
  - DropEdge

##### 总结

- 谱域神经网络的卷积定义在拉普拉斯矩阵的特征值矩阵上，这些卷积核参数无法迁移

#### 空域图神经网络

- 核心思想：在空域直接聚合邻接点信息

##### GraphSAGE

- SAGE：**SA**mple 和 aggre**G**at**E**
- Sample: 不使用所有邻居信息来更新节点，从2阶邻居种采样固定数量的邻居用于更新节点
- Aggregate: 通过聚合函数来聚合邻居节点信息
  - 均值聚合
  - LSTM聚合：先随机打乱邻居节点顺序，然后使用LSTM
  - 池化聚合：所有节点通过全连接层，然后经过最大池化

##### GAT

- $a_{ij}=\cfrac{exp(LaekyReLU(a[Wh_i||Wh_j]))}{\sum_{k\in N_i}exp(LaekyReLU(a[Wh_i||Wh_k]))}$
- $h_i^{t+1}=\sum_{j \in N_i} a_{ij}h_j^{t+1}$
- 多头拼接

##### 图同构网络

- $h_v^{t+1}=MLP((1+\epsilon^k) h_v^{t} + \sum_{u \in N_v}h_u^t)$
- 非常适用图分类任务

### 图神经网络的扩展

##### 深层图神经网络

- 残差链接
  - 与普通的残差连接不同，应该越靠近的邻居影响力越大
- JK-net(跳跃知识网络)
  - 将每层的输出，拼接起来作为最终的输出
- DropEdge

##### 图的池化

- 怎么从节点的嵌入得到图的嵌入，直接平均池化和最大池化会损失图的层级信息
- TopK池化，每次选取top K个重要节点，其他节点丢弃掉

##### FastGCN

###  图嵌入技术

#### 基于矩阵分解

- 拉普拉斯映射：对拉普拉斯矩阵进行特征值分解，然后取前n个最小非0特征值的特征向量

#### 基于随机游走

##### DeepWalk

- DeepWalk通过**随机游走**(truncated random walk)学习出一个网络的**表示**
- **采样**：通过随机游走对图上的节点进行采样，在给定的时间内得到一个节点构成的序列，论文研究表明从每个节点执行32到64次随机遍历就足够表示节点的结构关系；
- **训练skip-gram**：随机游走得到的节点序列与word2vec方法中的句子相当。文本中skip-gram的输入是一个句子，在这里输入为随机游走采样得到的序列，进一步通过最大化预测相邻节点的概率进行预测周围节点进行训练和学习。 通常预测大约20个邻居节点-左侧10个节点，右侧10个节点。
- **计算嵌入**

##### Node2vec

- Node2vec是DeepWalk的改进版，定义了一个bias random walk的策略生成序列，仍然用skip gram去训练。
- 该算法引入了参数P和Q，参数Q关注随机游走中未发现部分的可能性，即控制着游走是向外还是向内: 若Q>1，随机游走倾向于访问接近的顶点(偏向BFS); 若Q<1，倾向于访问远离的顶点(偏向DFS)。
- 参数P控制了随机游走返回到前一个节点的概率。也就是说，参数P控制节点局部关系的表示，参数Q控制较大邻域的关系。

#####  SDNE

- SDNE没有采用随机游走的方法而是使用自动编码器来同时优化一阶和二阶相似度，学习得到的向量表示保留局部和全局结构，并且对稀疏网络具有鲁棒性。

- 一阶相似度表征了边连接的成对节点之间的局部相似性。 如果网络中的两个节点相连，则认为它们是相似的。

- 二阶相似度表示节点邻域结构的相似性，它能够了表征全局的网络结构。 如果两个节点共享许多邻居，则它们趋于相似
- SDNE的具体做法是使用自动编码器来保持一阶和二阶网络邻近度。它通过联合优化这两个近似值来实现这一点。该方法利用高度非线性函数来获得嵌入。模型由两部分组成：无监督和监督。前者包括一个自动编码器，目的是寻找一个可以重构其邻域的节点的嵌入。后者基于拉普拉斯特征映射，当相似顶点在嵌入空间中彼此映射得很远时，该特征映射会受到惩罚

### 知识图谱

- 知识图谱：由实体(节点)和关系(不同类型的边)组成的多关系图
- 知识图谱一般由三元组表示$(h,r,t)$,每个事实包含两个实体和它们之间的关系
- 知识图谱的表示学习，是将实体和关系映射到一个低维连续空间上

#### 核心任务

- 利用已有知识对未知部分进行**推理**和**补全**
  - 比如已知h和t，求r，和已知h和r求t
- 对三元组进行分类，判断事实是否为真
- 实体分类
- 实体判别，判断两个尸体是否为同一个目标

#### 知识图谱嵌入方法

##### 距离变换模型

- TransE模型
  - 灵感来源于word2vec，对于关系三元组$(h,r,t)$,关系向量r看作头实体向量到尾实体向量的平移
    - 例如三元组(汤姆汉克斯, 出演，阿甘正传)和(姜文，出演，让子弹飞)，推导出：阿甘正传-汤姆汉克斯$\approx$让子弹飞-姜文​
  - 可以将r看作h到t的翻译
  - 打分函数定义为
    - $f_r(h,t)=-||h+r-t||_{L1/L2}$

- TransH模型
  - TransE模型简单高效，但是不能解决一对多和多对一关系，比如姜文在"出演"这个关系上和"汤姆汉克斯"接近，但是在其他关系上并不一定接近
  - TransH提出实体在不同关系下拥有不同的表示
  - 打分函数
    - $f_r(h,t)=-||h_{\perp}+r-t_{\perp}||_{2}^2$
    - $h_{\perp}=h-w^T_rhw_r$
    - $w_r$表示与向量r近似正交的向量，代表r所在的超平面

- TransR模型
  - TransR 和TransH思想类似，TransH仍然假设实体和关系在同一个语义空间，这限制了模型的表达，TransR定义了投影矩阵$M_r$
  - 打分函数为:
    - $f_r(h,t)=-||h_{\perp}+r-t_{\perp}||_{2}^2$
    - $h_{\perp}=M_r h$
    - $t_{\perp}=M_r t$

- TransD模型
  - TransR中头实体和尾实体定义了相同的投影矩阵，TransD定义了两个不同的投影矩阵
    - $h_{\perp}=M_{r1} h$
    - $t_{\perp}=M_{r2} t$
    - $M_{r1}$由一个对应关系的向量$w_r$和一个对应头实体的向量$w_h$组成,$M_{r1}=w_rw_h^T+I$
    - $M_{r2}$也类似,$M_{r2}=w_rw_t^T+I$

##### 语义匹配模型

通过匹配实体的潜在语义和向量空间表示中包含的关系来度量事实的可信性

- 语义匹配能量模型
  - $f_r(h,t)=g_u(r,h)^Tg_v(r,t)$
- 神经张量网络模型
  - $f_r(h,t)=r^Ttanh(h^TS_rt+M_r^1h+M_r^2h+b)$
  - 建模两个匹配关系:线性和双线性
  - 参数过多,不适用于大型知识图谱
- ConvE模型
  - $f_r(h,t)=\sigma(vec(\sigma([M_h,M_r]*w))W)t$
  - 容易训练,已经被应用到大规模知识图谱中

##### 知识图谱的图神经网络

- 关系图卷积网络

  - 节点更新

  - $h_i^{l+1}=\sigma(\sum_r\sum_{j \in N^t_i}\cfrac{1}{c_{i,r}}W^l_rh^l_j+W_0^lh_i^l)$

- CompGCN
  - 将传统的三元组关系作为要传递的信息,然后进行信息聚合和节点状态的更新
  - 考虑三种不同边的类型:有向边,反向边,自连边,定义三种投影矩阵$W_{rel} \in \{W_o,W_i,W_s\}$,关系映射向量为$h_r=W_{rel}r$
  - 节点更新表示
    - $h^{t+1}_u=f(\sigma(\sum_{(u,r)\in N(u)} W^l_r \phi(h_v,h_r^l)))$
    - $\phi$函数有是那种形式
      - $\phi(h_v,h_r)=h_v-h_r$
      - $\phi(h_v,h_r)=h_v*h_r$
      - $\phi(h_v,h_r)=h_v\star h_r$,循环相关操作

