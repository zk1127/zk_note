

### 线性分类

**线性回归$\xrightarrow[激活函数]{降维}$ 线性分类**

****

#### 线性分类

- 硬分类:$y \in \{0,1\}$
  - 线性判别分析
  - 感知机
- 软分类:$p(y=1)=p$
  - 生成式：高斯判别分析 Gaussian Discriminant Analysis
  - 判别式：逻辑斯蒂回归 Logistic Regression

#### 感知机

- 思想：错例驱动
- $f(x)=sign(w^Tx)，x\in \Bbb{R}^p,w \in \Bbb{R}^p$

- loss:被错误分类的点的个数
  - $L(w)=\sum_{i=1}^N I(y_i w^Tx_i < 0)$
  - $L(w)=\sum_{x \in D} -y_i w^Tx_i$,D表示分错样本的集合
- **pocket 算法**
  - 如果迭代更新后分类错误样本比前一次少，则更新权重系数 w ；
  - 没有减少则保持当前权重系数 w 不变。

#### 线性判别分析

- LDA（Linear Discriminant Analysis），Fisher 判别分析

- 思想

  - 将样本进行投影，以阈值为界
  - 目标是找到合适的投影方向：类内小，类间大

- 线性判别分析思路

  - 假定投影平面的法向量w,且$||w||=1$

    - $w^Tx$表示投影后的向量
    - 解释：$x\cdot w=|x||w|cos\theta$,结合$||w||=1$，故$w^Tx$表示投影后的向量

  - 假设为二分类问题

    - c1与c2投影后的均值和方差
      - $Z_1=\cfrac{1}{N_1} \sum_{i=1}^{N_1} w^Tx_i$ ,, $Z_2=\cfrac{1}{N_2} \sum_{i=1}^{N_2} w^Tx_i$
      - $S_1=\cfrac{1}{N_1} \sum_{i=1}^{N_1}(w^Tx_i-Z_1)(w^Tx_i-Z_1)^T$,,$S_1=\cfrac{1}{N_2} \sum_{i=1}^{N_2}(w^Tx_i-Z_2)(w^Tx_i-Z_2)^T$

    - 类内小，类间大的目标函数

      - $J(w)=\cfrac{(Z_1-Z_2)^2}{S_1+S_2}$
      - $\hat{w}=argmax_w J(w)$

      - $$
        分子=(\cfrac{1}{N_1} \sum_{i=1}^{N_1} w^Tx_i - \cfrac{1}{N_2} \sum_{i=1}^{N_2} w^Tx_i)^2 \\
        = [w^T(\sum_{i=1}^{N_1} x_i - \sum_{i=1}^{N_2} x_i)]^2\\
        = w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw
        $$

      - $$
        S_1=\cfrac{1}{N_1}  \sum_{i=1}^{N_1}(w^Tx_i-Z_1)(w^Tx_i-Z_1)^T\\
        = \cfrac{1}{N_1} \sum_{i=1}^{N_1} (w^Tx_i-\sum_{i=1}^{N_1}w^Tx_j) (w^Tx_i-\sum_{i=1}^{N_1}w^Tx_j)^T w\\
        w^T[\sum_{i=1}^{N_1}(x_i-\overline{x_{c_1}})(x_i-\overline{x_{c_1}})^T]w\\
        w^T \cdot S_{c_1} \cdot w
        $$

        

      - $$
        分母= w^T \cdot S_{c_1} \cdot w + w^T \cdot S_{c_2} \cdot w\\
        = w^T \cdot (S_{c_1} + S_{c_2}) \cdot w
        $$

      - $$
        J(w)=\cfrac{w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw}{w^T \cdot (S_{c_1} + S_{c_2}) \cdot w}
        $$

    - 模型求解
      - $w \propto  (S_{c_1} + S_{c_2})^{-1}(\overline{x_{c1}}-\overline{x_{c2}})$



