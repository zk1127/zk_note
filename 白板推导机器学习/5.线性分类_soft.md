### 线性分类

#### 逻辑斯蒂回归

- 软输出-概率判别模型，判别模型直接用MLE求解

- Sigmoid函数：$\sigma(z)=\cfrac{1}{1-e^{-z}}$

- 正例：$p_1=p(y=1|x)=\sigma(w^Tx)=\cfrac{1}{1+e^{-w^Tx}}$

- 负例：$p_0=p(y=0|x)=\sigma(w^Tx)=\cfrac{e^{-W^Tx}}{1+e^{-w^Tx}}$

- $p(y|x)=p_1^y p_0^{1-y}$

- $$
  MLE:\hat{w}=argmax_w logp(y|x)\\
  =argmax_w \sum_{i=1}^N y_i log(p_1)+(1-y_i)log(p_0)\\
  $$

- $MLE \Rightarrow cross\;\; entropy$



#### 高斯判别分析 Gaussian Discriminant Analysis

- 软输出-概率生成模型，使用贝叶斯定理求解

- 高斯判别分析的假设

  - $y \sim Bernoulli(\phi)\Rightarrow p(y=1)=\phi ; p(y=0)=1-\phi$
  - $x|y=1 \sim N(\mu_1,\Sigma)$,样本数为$N_1$
  - $x|y=0 \sim N(\mu_2,\Sigma)$,样本数为$N_2$
  - 两个$\Sigma$相同

- 高斯判别模型的联合概率的极大似然

  - 
    $$
    l(\theta)=log \prod_{i=1}^N p(x_i,y_i)\\
    =\sum_{i=1}^N logp(x_i|y_i)p(y_i)\\
    =\sum_{i=1}^N log(N(\mu_1,\Sigma)^{y_i})+log(N(\mu_2,\Sigma)^{1- y_i}) + log \phi^{y_i}(1-\phi)^{1-y_i}\\
    $$

  - $\theta=\{\mu_1,\mu_2,\phi,\Sigma\}$

  - $\hat{\theta}=argmax_{\theta} l(\theta)$

- 模型求解

  - 求$\phi$

    - $$
      \begin{align}
      \cfrac{\partial l(\theta)}{\partial \phi}
      &=\cfrac{\partial \sum_{i=1}^N y_i log\phi +(1-y_i)log(1-\phi)}{\partial \phi}\\
      &=\sum_{i=1}^N\cfrac{y_i}{\phi} -\cfrac{1-y_i}{1-\phi}=0\\
      &\Rightarrow \sum_{i=1}^N y_i(1-\phi)-(1-y_i)(\phi)=0\\
      &\Rightarrow \sum_{i=1}^N y_i-\phi=0\\
      &\Rightarrow \phi=\cfrac{1}{N} \sum_{i=1}^N y_i = \cfrac{N_1}{N}
      \end{align}
      $$
  
  - 求$\mu_1$
  
    - $$
      令\Delta=\sum_{i=1}^{N}log N(\mu_1,\Sigma)^{y_i}\\
      = \sum_{i=1}^{N}y_i log \cfrac{1}{(2\pi)^{p/2}|\Sigma|^{-1/2}}exp(-1/2(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu))\\
      去除无关项\\
      \Delta=\sum_{i=1}^{N}y_i(-1/2(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1))\\
      =-1/2\sum_{i=1}^{N}y_i(x_i^T\Sigma^{-1}x_i-2\mu_1\Sigma^{-1}x_i-\mu_1^T\Sigma^{-1}\mu_1)
      $$
  
    - 
      $$
      \cfrac{\partial \Delta}{\partial \mu_1}=-1/2\sum_{i=1}^{N}y_i(-2\Sigma^{-1}x_i-\mu_1^T\Sigma^{-1})=0\\
      \mu_1=\cfrac{\sum_{i=1}^Ny_ix_i}{\sum_{i=1}^Ny_i}=\cfrac{\sum_{i=1}^Ny_ix_i}{N_1}
      $$
  
  - $\mu_2$与之同理 
  
    - $\mu_2=\cfrac{\sum_{i=1}^N(1-y_i)x_i}{N_2}$
  
  - 求$\Sigma$
  
    - 高斯判别模型假设对正反例采用相同的协方差矩阵，当然从上面的求解中我们可以看到，即使采用不同的矩阵也不会影响之前的三个参数。首先我们有：
  
    - $$
      \begin{align*}
      \sum_{i=1}^N N(\mu,\Sigma)
      &=\sum_{i=1}^N log(\cfrac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}})+(-1/2(x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\\
      &=Const-1/2Nlog|\Sigma|-1/2 Trace((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\\
      &=Const-1/2Nlog|\Sigma|-1/2Trace((x_i-\mu)(x_i-\mu)^T\Sigma^{-1})\\
      &=Const-1/2Nlog|\Sigma|-1/2NTrace(S\Sigma^{-1})
      \end{align*}
      $$
  
    - 二次型是一个标量，这里引入迹可以交换矩阵的顺序，因为$Trace(AB)=Trace(BA)$
  
    - 对于包含绝对值和迹的表达式的导数，我们有:
  
    - 
      $$
      \cfrac{\partial(|A|)}{\partial A}=|A|A^{-1}\\
      \cfrac{\partial Trace(AB)}{\partial A}=B^{T}
      $$
  
    - 因此目标：
  
    - $$
      \begin{align}
      \sum_{i=1}^N y_ilog(N(\mu_1,\Sigma))+(1- y_i)log(N(\mu_2,\Sigma))\\
      =Const-1/2Nlog|\Sigma|-1/2N_1Trace(S_1\Sigma^{-1})-1/2N_2Trace(S_2\Sigma^{-1})
      \end{align}
      $$
  
    - 其中$S_1,S2$为两类数据内部的协方差，于是：
  
    - $$
      N\Sigma^{-1}-N_1S^T_{1}\Sigma^{-2}-N_2S^T_{2}\Sigma^{-2}=0\\
      \Rightarrow \Sigma=\cfrac{N_1S_1+N_2S_2}{N}
      $$
  
    - 这里应用了类协方差矩阵的对称性。

#### 朴素贝叶斯分类器 Navie Bayes Classifier

- 朴素贝叶斯假设- 条件独立假设，**属性独立**假设

- 最简单的概率图模型,有向图

- $x_i \perp x_j |y (i\neq j) \Rightarrow p(x|y)=\prod_{i=1}^{p}p(x_i|y)$

- 二分类：$y \sim Bernoulli\;Dist$

- 多分类：$y\sim Categrial \; Dist$

- 求解

- $$
  y=argmax_{y_i}P(y_i|x)\\
  p(y_i|x) \propto p(x|y_i)p(y_i)=p(x_1|y_i)p(x_2|y_i),....p(x_n|y_i)p(y_i)
  $$

  

